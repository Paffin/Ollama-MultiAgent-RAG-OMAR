Ниже приведён пример расширённого, структурированного и «красивого» файла README, который охватывает основные аспекты проекта: его назначение, функциональные возможности, установку, настройку и запуск.

---

# Мультиагентная система на Python с использованием Ollama, FAISS и Streamlit

## Описание проекта

Данный проект реализует мультиагентную архитектуру, где несколько специализированных агентов (Planner, Executor, Critic, Praise, Arbiter) совместно решают поставленные задачи, комбинируя возможности языковых моделей и локального поиска. Основные особенности проекта:

- **Интеграция с Ollama** – взаимодействие через REST API (эндпоинты `/api/tags` и `/api/generate`) для генерации ответов.
- **Векторная база с FAISS и SentenceTransformers** – поиск и агрегирование локальной информации (RAG).
- **Встроенные инструменты для работы с ОС** – выполнение системных команд, просмотр директорий, браузерные действия.
- **Многократные итерации до улучшения результата** – циклы типа Executor → Critic → Praise → Arbiter для доработки ответа.
- **Интерактивный веб-интерфейс на базе Streamlit** – удобное управление настройками агентов, параметрами генерации и загрузкой документов.

---

## Функциональные возможности

1. **Анализ запроса (PlannerAgent):**
   - Использует LLM для предварительного анализа пользовательского запроса.
   - Определяет, требуется ли интернет-поиск (ducksearch:), браузерное действие (browser:) или достаточно локальных данных.

2. **Выполнение инструкции (ExecutorAgent):**
   - Интерпретирует и выполняет команды: интернет-поиск, выполнение системных команд, работу с браузером, поиск по векторной базе и даже анализ визуального контента.
   - При отсутствии явной команды генерирует ответ с использованием языковой модели.

3. **Анализ качества ответа (CriticAgent и PraiseAgent):**
   - **CriticAgent** – выявляет ошибки, слабые места и недостающую информацию.
   - **PraiseAgent** – отмечает сильные стороны и положительные моменты ответа.

4. **Формирование доработки (ArbiterAgent):**
   - На основе полученной критики и похвалы создаёт инструкцию для доработки ответа без полной переработки.

5. **Интерактивный интерфейс и лог цепочки (Streamlit):**
   - Возможность загрузки текстовых документов для RAG-поиска.
   - Настройка параметров генерации (temperature, top_p и т.д.) и выбор моделей Ollama для каждого агента.
   - Пошаговый вывод цепочки сообщений (chain-of-thought) для прозрачности работы системы.

---

## Структура проекта

```
multiagent_system/
├── agents.py           # Реализация классов агентов (Planner, Executor, Critic, Praise, Arbiter)
├── ollama_client.py    # Клиент для взаимодействия с Ollama (методы list_models, generate)
├── rag_db.py           # Простая векторная база данных на основе FAISS и SentenceTransformers
├── streamlit_app.py    # Основное приложение Streamlit с настройками и логикой итераций
├── windows_tools.py    # Инструменты для работы с ОС: выполнение команд, действия в браузере и т.д.
├── requirements.txt    # Список зависимостей (streamlit, requests, faiss-cpu, sentence-transformers и др.)
└── README.md           # (Этот файл)
```

---

## Установка и настройка

### 1. Установка Ollama

- Скачайте и установите [Ollama](https://github.com/jmorganca/ollama).  
- Запустите сервер Ollama на порту **11434**:
  ```bash
  ollama serve --port 11434
  ```
- Проверьте работу сервера:
  ```bash
  curl http://localhost:11434/api/tags
  ```
  Ответ должен содержать JSON со списком доступных моделей.

### 2. Клонирование репозитория и установка зависимостей

1. **Клонируйте проект:**
   ```bash
   git clone https://github.com/youruser/your-repo.git
   cd your-repo
   ```

2. **Создайте и активируйте виртуальное окружение (рекомендуется):**
   - На Linux/macOS:
     ```bash
     python3 -m venv venv
     source venv/bin/activate
     ```
   - На Windows:
     ```bash
     python -m venv venv
     .\venv\Scripts\activate
     ```

3. **Установите зависимости:**
   ```bash
   python -m pip install --upgrade pip
   pip install -r requirements.txt
   ```
   _Примечание_: Если возникают проблемы с установкой `faiss-cpu` на Windows, попробуйте использовать другую сборку через Conda или найдите подходящую версию на PyPi.

---

## Запуск приложения

### 1. Убедитесь, что Ollama запущен

В отдельном терминале запустите:
```bash
ollama serve --port 11434
```

### 2. Запуск веб-интерфейса Streamlit

Запустите приложение командой:
```bash
streamlit run streamlit_app.py
```
Откройте браузер и перейдите по адресу, указанному Streamlit (обычно `http://localhost:8501`).

---

## Использование приложения

### Интерфейс и настройка агентов

- **Настройка моделей и системных промптов:**
  - Выберите модели Ollama для каждого агента (Planner, Executor, Critic, Praise, Arbiter) из списка, полученного через API Ollama.
  - Отредактируйте системные промпты для каждого агента, чтобы задать стиль и роль (например, указать, что Planner должен анализировать запрос и предлагать инструкции).
  - Нажмите кнопку «Инициализировать агентов», чтобы сохранить настройки.

- **Загрузка документов (для RAG):**
  - В разделе «Загрузка документов (RAG)» можно добавить текстовые файлы. Они будут индексированы с помощью FAISS, и система сможет проводить поиск по ним.

- **Настройка параметров Ollama:**
  - В боковой панели установите параметры генерации (temperature, top_p, presence_penalty, frequency_penalty, num_ctx, num_predict).
  - Задайте количество итераций (1–5) для цикла доработки ответа.

### Процесс выполнения запроса

1. **Ввод запроса:**
   - В главном окне введите текст запроса или задачу.
   - При нажатии на кнопку «Запустить» начинается цепочка действий:
     - **PlannerAgent** анализирует запрос и генерирует инструкцию.
     - **ExecutorAgent** выполняет инструкцию: может быть вызван интернет-поиск, действия в браузере, локальный поиск и т.д.
     - **CriticAgent** и **PraiseAgent** оценивают качество ответа.
     - **ArbiterAgent** формирует инструкцию по доработке, после чего Executor повторно улучшает ответ.
2. **Отображение цепочки (Chain-of-Thought):**
   - На странице отображается подробный лог всех шагов, что позволяет отследить работу каждого агента.

3. **Получение финального ответа:**
   - После завершения всех итераций финальный ответ ExecutorAgent выводится на экран.
   - При необходимости можно запустить дополнительный цикл доработки.

---

## Возможные расширения

- **Интеграция дополнительных инструментов:**
  - Расширение возможностей `windows_tools.py` для работы с PowerShell, PDF-файлами и другими форматами.
- **Динамическая остановка итераций:**
  - Автоматический выход из цикла, если CriticAgent не находит значимых ошибок.
- **Расширение агентской цепочки:**
  - Добавление дополнительных агентов (например, агент-тестировщик или агент-программист) для более сложных сценариев.
- **Сохранение логов:**
  - Возможность сохранения истории chain-of-thought в базу данных (SQLite, MongoDB) для последующего анализа.
- **Интеграция в другие фреймворки:**
  - Перенос логики в Flask, FastAPI или создание CLI-интерфейса.

---

## Частые проблемы и рекомендации

- **Ошибка 404 Not Found на `/api/generate`:**
  - Проверьте, что Ollama действительно предоставляет эндпоинт `/api/generate` (в некоторых версиях может быть `/v1/generate`).
- **Проблемы с `faiss-cpu` на Windows:**
  - Используйте сборку, рекомендованную для вашей версии Python, либо попробуйте установить через Conda.
- **Задержки при работе с большими моделями:**
  - Снизьте значения параметров `num_predict` и `temperature` или используйте более компактные модели.
- **Streamlit ошибки (например, с высотой компонента):**
  - Настройте параметры компонентов (например, `height` в `st.text_area`) согласно рекомендациям Streamlit.

---

## Лицензия и авторы

- **Лицензия:** Код распространяется на свободных условиях. Для Ollama, FAISS и SentenceTransformers см. соответствующие лицензии на GitHub.
- **Авторы:** Основная разработка – Максим Киктев.